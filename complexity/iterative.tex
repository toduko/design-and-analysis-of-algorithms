\section{Как анализираме един алгоритъм по сложност?}

Нека започнем с един прост пример:
\lstinputlisting{algorithms/find.txt}

Да кажем, че искаме да проверим броя на инструкциите, която тази функция ще изпълни, преди да приключи работата си.
Точен отговор не може да се даде.
В зависимост от това къде се намира $v$ във $A[1 \dots n]$, алгоритъмът може да приключи много бързо или много бавно.
Можем да дадем горна и долна граница на бързодействието.

Ако $v$ се намира в началото, то ще сме направили само следните $4$ операции:
\begin{itemize}
  \item да инициализираме променливата $i$ със $0$;
  \item да проверим верността на $i < n$;
  \item да проверим верността на $A[i] = v$;
  \item да върнем $i$ т.е. $1$.
\end{itemize}

Нека сега да помислим какво ще стане в най-лошия случай (обикновено от тези ще се интересуваме) -- $v$ не участва в $A[1 \dots n]$.
Тогава $n$ пъти ще изпълним следните $3$ операции:
\begin{itemize}
  \item проверяваме верността на $i \leq n$;
  \item проверяваме верността на $A[i] = v$;
  \item увеличаваме $i$ с $1$.
\end{itemize}
Освен тези $3n$ операции, преди всичко трябва да инициализираме променливата $i$ със $1$, да се направи последната проверка на верността на $i \leq n$ (която ще ни изкара от цикъла), и да върнем $-1$.
Общо излизат $3n + 3$ операции.

Така виждаме, че в зависимост от входните данни, алгоритъмът приключва работа за поне $4$ стъпки и най-много $3n + 3$ стъпки.
Такъв алгоритъм ще казваме, че има сложност по време $O(n)$.
Разбира се, няма да е грешно и да кажем, че алгоритъмът има сложност по време $\Omega(1)$, но това не ни дава никаква информация, защото всеки алгоритъм има такава сложност.
Също така, понеже не използваме допълнителни променливи, алгоритъмът ни има константна сложност по памет или сложност по памет $\Theta(1)$.

По-общо казано, се интересуваме от асимптотиката на $T(n)$, където $T(n)$ е броят елементарни инструкции, които алгоритъмът извиква по време на своето изпълнение, при вход с размер $n$ в най-лошият случай.

Тук вход с големина $n$ може да означава различни неща.
Ако входът е някакъв масив или множество, то под размер ще разбираме броят на елементи.
Ако пък входът е число, то под размер можем да разбираме самата стойност на числото или дължината на двоичния запис.

\section{Предимствата и недостатъците на този вид анализ}

Най-голямото предимство на асимптотичния анализ, е неговата простота.
Вместо да влачим някакви константни множители и събираеми, имаме колкото се може по-проста формула, която да описва сложността на нашия алгоритъм.
Това дали един алгоритъм работи със две или три стъпки по-бързо/бавно не ни интересува особено много.
При много голям вход те ще работят практически еднакво.
В някакъв смисъл това ни помага да виждаме по-голямата картинка.
Един алгоритъм може да бъде по-бърз от друг, но от по-бърз алгоритъм до по-бърз алгоритъм има голяма разлика.

Нека вземем за пример следната таблица:
\begin{center}
  \begin{tabular}{|c|c|c|c|c|}
    \hline
    $n$       & $\lceil\log_2(n)\rceil$ & $n$       & $n^2$           & $2^n$                    \\
    \hline
    $1$       & $0$                     & $1$       & $1$             & $2$                      \\
    \hline
    $10$      & $4$                     & $10$      & $100$           & $1024$                   \\
    \hline
    $100$     & $7$                     & $100$     & $10000$         & число със $31$ цифри     \\
    \hline
    $10000$   & $13$                    & $10000$   & $100000000$     & число със $3011$ цифри   \\
    \hline
    $1000000$ & $20$                    & $1000000$ & $1000000000000$ & число със $301030$ цифри \\
    \hline
  \end{tabular}
\end{center}

Алгоритъм със сложността $n^2$ ще е по-бавен от алгоритъм със сложност $n$, обаче скока в бързината е много по-малък от този между $2^n$ и $n^2$.

Този подход обаче си има своите недостатъци.
Нека разгледаме два алгоритъма със сложности по време съответно $n$ и $2^{2^{2^{2^{1024}}}}$.
Ние ведната ще се втурнем да кажем, че първият алгоритъм е по-лош.
Той е с линейна сложност, а вторият алгоритъм има константна сложност.
Обаче преди вторият алгоритъм даде отговор, всички звезди ще умрат т.е. няма да доживем да чуем този отговор.
Разбира се, от някъде нататък, за много голями входни данни, първият алгоритъм наистина ще работи по-бавно, но ние никога няма да работим с толкова големи данни.
Тогава на практика, първият алгоритъм е по-добър, нищо че асимптотично се води по-лош.
Нас това няма да ни интересува в курса по ДАА.

\section{Сложност по време на някои итеративни алгоритми}

Нека видим сложността на алгоритъма за сортиране по метода на мехурчето:
\lstinputlisting{algorithms/bubble-sort.txt}

В най-лошия случай сложността $T(n)$ на функцията {\tt Sort} е следната:
\[
  T(n) = \sum\limits_{i = 1}^{n - 1} \sum\limits_{j = 1}^{n - i - 1} 1 = \sum\limits_{i = 1}^{n - 1} (n - i - 1) = (n - 2) + (n - 3) + \dots + 0 = \frac{(n - 2)(n - 1)}{2} \asymp n^2.
\]

По принцип $T(n)$ трябва да е сума от $4$, а не от $1$, но такъв константен брой операции, дори и приложени неконстантен брой пъти, не влияят на асимптотичното поведение.

Нека сега разгледаме следният алгоритъм за степенуване:
\lstinputlisting{algorithms/exp.txt}

Той се възползва от простата идея, че за да сметнем да кажем $3^8$, можем вместо $8$ пъти да умножаваме числото $3$, да представим $3^8$ като $3^4 \cdot 3^4$.
Тогава $3^4$ можем да сметнем веднъж, и да го умножим със себе си.
Пак можем да представим $3^4$ като $3^2 \cdot 3^2$ и да пресметнем $3^2$ само веднъж и да го умножим със себе си.
Така при по-голяма стойност на $y$ си спестяваме много работа.

С уговорката, че умножението е атомарна операция, сложността по време $T(n)$ ($n$ е стойността на $y$) на функцията {\tt Exp} е следната:
\[
  T(n) = \sum\limits_{\substack{i = n \\ i \leftarrow \frac{i}{2}}}^1 1 = \underbrace{1 + \dots + 1}_{\substack{\text{колкото пъти} \\\text{можем да} \\ \text{делим целочислено} \\ n \text{ на } 2 \text{ преди} \\ \text{да получим } 0}} = \underbrace{1 + \dots + 1}_{\text{около } \log(n) \text{ пъти}} \asymp \log(n).
\]

За да можем по-формално да изследваме този вид поведение, ще трябва да си поиграем малко с рекурентни уравнения.

\newpage